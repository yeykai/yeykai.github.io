<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.gif">
  <link rel="icon" type="image/png" href="/img/favicon.gif">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="我的学习日记">
  <meta name="author" content="YeYKai">
  <meta name="keywords" content="">
  <title>【学习日记】第一个爬虫---Scrapy框架入门(一) ~ YeYKai的博客</title>
  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css?v=5.7.2">
  <link rel="stylesheet" href="/lib/mdbootstrap/css/bootstrap.min.css?v=4.3.1">
  <link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css?v=4.8.7">
  <link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css">
  <link rel="stylesheet" href="/lib/nprogress/nprogress.css?v=0.2.0">
  <link rel="stylesheet" href="https://at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">
  
    <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css">
  
  <link rel="stylesheet" href="/css/main.css">

  
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>YeYKai的博客</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2"
         style="background: url('http://img.yeykai.cn/img/post.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              <p>星期三, 八月 28日 2019, 4:49 下午</p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto">
          <div class="markdown-body">
            <h4 id="爬虫是一个非常好用的数据采集工具，数据的来源就是庞大的互联网资源，它的存在让你节省很多手动的繁琐的数据搜集的时间，而且它的实现也并不复杂，几十行代码就能轻松地在数秒内采集大量数据。"><a href="#爬虫是一个非常好用的数据采集工具，数据的来源就是庞大的互联网资源，它的存在让你节省很多手动的繁琐的数据搜集的时间，而且它的实现也并不复杂，几十行代码就能轻松地在数秒内采集大量数据。" class="headerlink" title="爬虫是一个非常好用的数据采集工具，数据的来源就是庞大的互联网资源，它的存在让你节省很多手动的繁琐的数据搜集的时间，而且它的实现也并不复杂，几十行代码就能轻松地在数秒内采集大量数据。"></a>爬虫是一个非常好用的数据采集工具，数据的来源就是庞大的互联网资源，它的存在让你节省很多手动的繁琐的数据搜集的时间，而且它的实现也并不复杂，几十行代码就能轻松地在数秒内采集大量数据。</h4><a id="more"></a>
<h4 id="关于爬虫的实现，我选择python语言来操作，虽然java也是可以做到类似的功能，但是我也想体验一下python的开发过程。简单的一个python爬虫，可以使用request库来完成，它可以帮助你完成一系列网络请求，并返回一个response对象，包含网页的内容，就可以开始对网页内容使用xpath等工具得到你想要的信息。"><a href="#关于爬虫的实现，我选择python语言来操作，虽然java也是可以做到类似的功能，但是我也想体验一下python的开发过程。简单的一个python爬虫，可以使用request库来完成，它可以帮助你完成一系列网络请求，并返回一个response对象，包含网页的内容，就可以开始对网页内容使用xpath等工具得到你想要的信息。" class="headerlink" title="关于爬虫的实现，我选择python语言来操作，虽然java也是可以做到类似的功能，但是我也想体验一下python的开发过程。简单的一个python爬虫，可以使用request库来完成，它可以帮助你完成一系列网络请求，并返回一个response对象，包含网页的内容，就可以开始对网页内容使用xpath等工具得到你想要的信息。"></a>关于爬虫的实现，我选择python语言来操作，虽然java也是可以做到类似的功能，但是我也想体验一下python的开发过程。简单的一个python爬虫，可以使用request库来完成，它可以帮助你完成一系列网络请求，并返回一个response对象，包含网页的内容，就可以开始对网页内容使用xpath等工具得到你想要的信息。</h4><h4 id="不过今天的主角是一个爬虫框架—Scrapy，是基于twisted框架（一个流行的事件驱动的python网络框架）开发而来，它内部提供的各个组件相互协作，完成页面请求下载，数据清洗存档等一些列操作。下图是他的运作模型图："><a href="#不过今天的主角是一个爬虫框架—Scrapy，是基于twisted框架（一个流行的事件驱动的python网络框架）开发而来，它内部提供的各个组件相互协作，完成页面请求下载，数据清洗存档等一些列操作。下图是他的运作模型图：" class="headerlink" title="不过今天的主角是一个爬虫框架—Scrapy，是基于twisted框架（一个流行的事件驱动的python网络框架）开发而来，它内部提供的各个组件相互协作，完成页面请求下载，数据清洗存档等一些列操作。下图是他的运作模型图："></a>不过今天的主角是一个爬虫框架—Scrapy，是基于twisted框架（一个流行的事件驱动的python网络框架）开发而来，它内部提供的各个组件相互协作，完成页面请求下载，数据清洗存档等一些列操作。下图是他的运作模型图：</h4><p><img src="http://img.yeykai.cn/img/scrapy架构.jpg" alt="scrapy架构图"></p>
<h4 id="这里简单介绍一下上图的几个重要组件："><a href="#这里简单介绍一下上图的几个重要组件：" class="headerlink" title="这里简单介绍一下上图的几个重要组件："></a>这里简单介绍一下上图的几个重要组件：</h4><ol>
<li>ScrapyEngine:整个框架的中枢，为各个组件传递请求或响应数据</li>
<li>Spiders:自定义的页面数据处理逻辑</li>
<li>Scheduler:url的调度器</li>
<li>ItemPipeline:对Spiders处理好的数据做进一步处理，如入库或写入文件</li>
</ol>
<h4 id="我也是今天刚接触这一框架，有理解不对的地方各位大佬在评论去留个言，这里用一个简单的例子，来应用scrapy框架。我将写一个爬取牛客网的剑指offer题库的题目信息，包括每个题的类型，通过率等信息，最终输出到csv表格中。"><a href="#我也是今天刚接触这一框架，有理解不对的地方各位大佬在评论去留个言，这里用一个简单的例子，来应用scrapy框架。我将写一个爬取牛客网的剑指offer题库的题目信息，包括每个题的类型，通过率等信息，最终输出到csv表格中。" class="headerlink" title="我也是今天刚接触这一框架，有理解不对的地方各位大佬在评论去留个言，这里用一个简单的例子，来应用scrapy框架。我将写一个爬取牛客网的剑指offer题库的题目信息，包括每个题的类型，通过率等信息，最终输出到csv表格中。"></a>我也是今天刚接触这一框架，有理解不对的地方各位大佬在评论去留个言，这里用一个简单的例子，来应用scrapy框架。我将写一个爬取牛客网的剑指offer题库的题目信息，包括每个题的类型，通过率等信息，最终输出到csv表格中。</h4><h4 id="写爬虫的第一步，当然是需要看看要爬取的资源长什么样，才能进行后续的代码编写，我们找到目标地址，打开浏览器的开发者工具，查看我们要的数据在哪个位置，下图便是我要爬取的目标数据的位置："><a href="#写爬虫的第一步，当然是需要看看要爬取的资源长什么样，才能进行后续的代码编写，我们找到目标地址，打开浏览器的开发者工具，查看我们要的数据在哪个位置，下图便是我要爬取的目标数据的位置：" class="headerlink" title="写爬虫的第一步，当然是需要看看要爬取的资源长什么样，才能进行后续的代码编写，我们找到目标地址，打开浏览器的开发者工具，查看我们要的数据在哪个位置，下图便是我要爬取的目标数据的位置："></a>写爬虫的第一步，当然是需要看看要爬取的资源长什么样，才能进行后续的代码编写，我们找到目标地址，打开浏览器的开发者工具，查看我们要的数据在哪个位置，下图便是我要爬取的目标数据的位置：</h4><p><img src="http://img.yeykai.cn/img/pachong.jpg" alt="目标资源定位"></p>
<h4 id="确认过眼神，是能爬到的数据，开始搭建工程搞起，首先，在本地安装scrapy，只需要用pip-install-scrapy命令下载即可，如果出现下载失败网络超时的现象，可以尝试使用国内镜像下载，当然，如果你使用PyCharm开发，也可以直接修改镜像来源，如下图所示："><a href="#确认过眼神，是能爬到的数据，开始搭建工程搞起，首先，在本地安装scrapy，只需要用pip-install-scrapy命令下载即可，如果出现下载失败网络超时的现象，可以尝试使用国内镜像下载，当然，如果你使用PyCharm开发，也可以直接修改镜像来源，如下图所示：" class="headerlink" title="确认过眼神，是能爬到的数据，开始搭建工程搞起，首先，在本地安装scrapy，只需要用pip install scrapy命令下载即可，如果出现下载失败网络超时的现象，可以尝试使用国内镜像下载，当然，如果你使用PyCharm开发，也可以直接修改镜像来源，如下图所示："></a>确认过眼神，是能爬到的数据，开始搭建工程搞起，首先，在本地安装scrapy，只需要用pip install scrapy命令下载即可，如果出现下载失败网络超时的现象，可以尝试使用国内镜像下载，当然，如果你使用PyCharm开发，也可以直接修改镜像来源，如下图所示：</h4><p><img src="http://img.yeykai.cn/img/mirror.png" alt="PyCharm镜像设置"></p>
<h4 id="安装完成后，找一个目录，用命令行输入scrapy-startproject-yourprojectname来创建一个scrapy工程，创建后得到这样的目录："><a href="#安装完成后，找一个目录，用命令行输入scrapy-startproject-yourprojectname来创建一个scrapy工程，创建后得到这样的目录：" class="headerlink" title="安装完成后，找一个目录，用命令行输入scrapy startproject yourprojectname来创建一个scrapy工程，创建后得到这样的目录："></a>安装完成后，找一个目录，用命令行输入scrapy startproject yourprojectname来创建一个scrapy工程，创建后得到这样的目录：</h4><p><img src="http://img.yeykai.cn/img/cmdscrapy.jpg" alt="命令行创建scrapy工程"></p>
<p><img src="http://img.yeykai.cn/img/mulv.jpg" alt="创建后目录"></p>
<h4 id="使用PyCharm打开这个工程，这里生成的文件，根据文件名结合前面的架构图就能知道它们大概的作用。开发前，我们需要给这个工程加一个启动文件，因为scrapy默认是需要在命令行运行的，所以我们编写一个入口文件在根目录下，模拟命令行操作，并将他设置为启动文件，具体代码如下："><a href="#使用PyCharm打开这个工程，这里生成的文件，根据文件名结合前面的架构图就能知道它们大概的作用。开发前，我们需要给这个工程加一个启动文件，因为scrapy默认是需要在命令行运行的，所以我们编写一个入口文件在根目录下，模拟命令行操作，并将他设置为启动文件，具体代码如下：" class="headerlink" title="使用PyCharm打开这个工程，这里生成的文件，根据文件名结合前面的架构图就能知道它们大概的作用。开发前，我们需要给这个工程加一个启动文件，因为scrapy默认是需要在命令行运行的，所以我们编写一个入口文件在根目录下，模拟命令行操作，并将他设置为启动文件，具体代码如下："></a>使用PyCharm打开这个工程，这里生成的文件，根据文件名结合前面的架构图就能知道它们大概的作用。开发前，我们需要给这个工程加一个启动文件，因为scrapy默认是需要在命令行运行的，所以我们编写一个入口文件在根目录下，模拟命令行操作，并将他设置为启动文件，具体代码如下：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line">execute([<span class="string">'scrapy'</span>, <span class="string">'crawl'</span>, <span class="string">'spidersTest'</span>])</span><br></pre></td></tr></table></figure>
<h4 id="其中这里的第三个参数spidersTest是你要启动的爬虫名字，不能弄错，到此开发环境基本搭建完毕，可以开始正式开发了，具体的开发步骤，我根据中文文档来操作。"><a href="#其中这里的第三个参数spidersTest是你要启动的爬虫名字，不能弄错，到此开发环境基本搭建完毕，可以开始正式开发了，具体的开发步骤，我根据中文文档来操作。" class="headerlink" title="其中这里的第三个参数spidersTest是你要启动的爬虫名字，不能弄错，到此开发环境基本搭建完毕，可以开始正式开发了，具体的开发步骤，我根据中文文档来操作。"></a>其中这里的第三个参数spidersTest是你要启动的爬虫名字，不能弄错，到此开发环境基本搭建完毕，可以开始正式开发了，具体的开发步骤，我根据<a href="https://scrapy-chs.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">中文文档</a>来操作。</h4><h4 id="第一步，我们写一个简单的item类，定义变量来存放我们采集到的数据，具体代码如下："><a href="#第一步，我们写一个简单的item类，定义变量来存放我们采集到的数据，具体代码如下：" class="headerlink" title="第一步，我们写一个简单的item类，定义变量来存放我们采集到的数据，具体代码如下："></a>第一步，我们写一个简单的item类，定义变量来存放我们采集到的数据，具体代码如下：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyscrapyItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    kaodian = scrapy.Field()</span><br><span class="line">    question = scrapy.Field()</span><br><span class="line">    hot = scrapy.Field()</span><br><span class="line">    passingRate = scrapy.Field()</span><br></pre></td></tr></table></figure>
<h4 id="第二步，也是非常关键的一步，编写自定义的spider类，来爬取和处理数据，下面是具体的实现代码："><a href="#第二步，也是非常关键的一步，编写自定义的spider类，来爬取和处理数据，下面是具体的实现代码：" class="headerlink" title="第二步，也是非常关键的一步，编写自定义的spider类，来爬取和处理数据，下面是具体的实现代码："></a>第二步，也是非常关键的一步，编写自定义的spider类，来爬取和处理数据，下面是具体的实现代码：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Selector, Request</span><br><span class="line"><span class="keyword">from</span> myscrapy.items <span class="keyword">import</span> MyscrapyItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">spidersTest</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'spidersTest'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.nowcoder.com'</span>]</span><br><span class="line">    baseUrls = <span class="string">'https://www.nowcoder.com/ta/coding-interviews?query=&amp;asc=true&amp;order=&amp;page='</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">            url = self.baseUrls + str(i)</span><br><span class="line">            <span class="keyword">yield</span> Request(url,self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">        item = MyscrapyItem()</span><br><span class="line"></span><br><span class="line">        sel = Selector(response)  <span class="comment"># Xpath选择器</span></span><br><span class="line">        kaodians = sel.xpath(<span class="string">'//td[@class="offer-pot txt-left"]/text()'</span>).re(<span class="string">'\S+'</span>)</span><br><span class="line">        questions = sel.xpath(<span class="string">'//td[@class="txt-left"]/a/text()'</span>).re(<span class="string">'\S+'</span>)</span><br><span class="line">        hots = sel.xpath(<span class="string">'//tr/td[3]/text()'</span>).extract()</span><br><span class="line">        passingRates = sel.xpath(<span class="string">'//tr/td[4]/text()'</span>).extract()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span>  range(<span class="number">0</span>, len(kaodians)):</span><br><span class="line">            item[<span class="string">'kaodian'</span>] = kaodians[i]</span><br><span class="line">            item[<span class="string">'question'</span>] = questions[i]</span><br><span class="line">            item[<span class="string">'hot'</span>] = hots[i]</span><br><span class="line">            item[<span class="string">'passingRate'</span>] = passingRates[i]</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<h4 id="下面是这个类的实现细节："><a href="#下面是这个类的实现细节：" class="headerlink" title="下面是这个类的实现细节："></a>下面是这个类的实现细节：</h4><ol>
<li>首先它需要继承Spider，这样它才是一个spider。</li>
<li>name变量代表这个爬虫的名字，也是用命令行执行的名字</li>
<li>allowed_domains表示要爬取的网页需要在这个域名下</li>
<li>start_requests(self)定义我们要爬取的页面url，并以 parse 为回调函数生成 Request，这里我们要爬取的题目有4页，所以需要生成4个url</li>
<li>parse(self, response)负责处理response并返回处理的数据以及(/或)跟进的URL。</li>
<li>在parse方法中，我们需要开始处理response，在整个页面数据中取到我们想要的数据，所以需要用到一个选择器。筛选的方式有很多，理论上xpath和正则表达式都是能实现的，但是对于这种返回的html页面，现在xpath来解析会更加方便，正则表达式可以作为辅助，对xpath解析的数据做进一步清洗。</li>
<li>常用的选择器还有BeautifulSoup和lxml，具体用法这里就不说了，自行百度哈哈</li>
<li>从前面的浏览器控制台，我们知道考点和题目字段可以直接从td的类名获取到，xpath表达式也只需要用//绝对路径去找就好了，而对于热度和通过率字段，这两个td是没类名的，因此可以用索引来寻找，也很简单，最后获取到之后需要加上extract()方法转换成字符串，而对于考点和题目字段，提取后还会有换行符和空格的情况，这是可以用re()方法，利用正则表达式’\S+’获取非空的信息。</li>
<li>最后，把获取的数据放在前面定义的item中，等待itempipeline进一步处理</li>
</ol>
<h4 id="最后一步，编写itempipeline，将数据写入到表格中，这里需要先用-init-self-方法初始化表格文件，真正处理的方法在process-item-self-item-spider-中，具体代码如下："><a href="#最后一步，编写itempipeline，将数据写入到表格中，这里需要先用-init-self-方法初始化表格文件，真正处理的方法在process-item-self-item-spider-中，具体代码如下：" class="headerlink" title="最后一步，编写itempipeline，将数据写入到表格中，这里需要先用 init(self)方法初始化表格文件，真正处理的方法在process_item(self, item, spider)中，具体代码如下："></a>最后一步，编写itempipeline，将数据写入到表格中，这里需要先用 <strong>init</strong>(self)方法初始化表格文件，真正处理的方法在process_item(self, item, spider)中，具体代码如下：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyscrapyPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># csv文件的位置,无需事先创建</span></span><br><span class="line">        store_file = <span class="string">'D:/spiders/newcoder.csv'</span></span><br><span class="line">        <span class="comment"># 打开(创建)文件</span></span><br><span class="line">        self.file = open(store_file, <span class="string">'w'</span>,newline=<span class="string">''</span>)</span><br><span class="line">        <span class="comment"># csv写法</span></span><br><span class="line">        self.writer = csv.writer(self.file)</span><br><span class="line">        <span class="comment"># 写入表头</span></span><br><span class="line">        self.writer.writerow((<span class="string">'考点'</span>.encode(<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>),</span><br><span class="line">                              <span class="string">'题目'</span>.encode(<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>),</span><br><span class="line">                              <span class="string">'热度'</span>.encode(<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>),</span><br><span class="line">                              <span class="string">'通过率'</span>.encode(<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 数据写入csv</span></span><br><span class="line">        self.writer.writerow((item[<span class="string">'kaodian'</span>].encode(<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>),</span><br><span class="line">                              item[<span class="string">'question'</span>].encode(<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>),</span><br><span class="line">                              item[<span class="string">'hot'</span>].encode(<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>),</span><br><span class="line">                              item[<span class="string">'passingRate'</span>].encode(<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>)))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 关闭爬虫时顺便将文件保存退出</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure>
<h4 id="运行前，需要把配置文件setting-py的下面这段代码取消注释"><a href="#运行前，需要把配置文件setting-py的下面这段代码取消注释" class="headerlink" title="运行前，需要把配置文件setting.py的下面这段代码取消注释"></a>运行前，需要把配置文件setting.py的下面这段代码取消注释</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'myscrapy.pipelines.MyscrapyPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="到此，整个开发过程完成，可以测试一下成果了，下图是测试结果："><a href="#到此，整个开发过程完成，可以测试一下成果了，下图是测试结果：" class="headerlink" title="到此，整个开发过程完成，可以测试一下成果了，下图是测试结果："></a>到此，整个开发过程完成，可以测试一下成果了，下图是测试结果：</h4><p><img src="http://img.yeykai.cn/img/ceshijieguo.png" alt="运行日志"></p>
<p><img src="http://img.yeykai.cn/img/csvjieguo.jpg" alt="输出表格内容"></p>
<h4 id="可以看到，短短一秒就将大量数据从网上成功地爬出来，效果很好，达到预期目标。今天很开心，第一次使用scrapy写爬虫，虽然案例很简单，但是整个过程下来，还是觉得很好玩，很有意思，因此写下本文记录一下，总结经验。如果有哪里不对，请大佬们指点一下。"><a href="#可以看到，短短一秒就将大量数据从网上成功地爬出来，效果很好，达到预期目标。今天很开心，第一次使用scrapy写爬虫，虽然案例很简单，但是整个过程下来，还是觉得很好玩，很有意思，因此写下本文记录一下，总结经验。如果有哪里不对，请大佬们指点一下。" class="headerlink" title="可以看到，短短一秒就将大量数据从网上成功地爬出来，效果很好，达到预期目标。今天很开心，第一次使用scrapy写爬虫，虽然案例很简单，但是整个过程下来，还是觉得很好玩，很有意思，因此写下本文记录一下，总结经验。如果有哪里不对，请大佬们指点一下。"></a>可以看到，短短一秒就将大量数据从网上成功地爬出来，效果很好，达到预期目标。今天很开心，第一次使用scrapy写爬虫，虽然案例很简单，但是整个过程下来，还是觉得很好玩，很有意思，因此写下本文记录一下，总结经验。如果有哪里不对，请大佬们指点一下。</h4>
            <hr>
          </div>
          <br>
          <div>
            
              <p>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0">学习日记</a>
                  &nbsp;
                
              </p>
            
            <p>
              <i class="iconfont icon-tag"></i>
              
                <a class="hover-with-bg" href="/tags/Python">Python</a>
              
                <a class="hover-with-bg" href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>
              
                <a class="hover-with-bg" href="/tags/Scrapy">Scrapy</a>
              
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" rel="nofollow noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
    <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
  </a>

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

      <a href="http://beian.miit.gov.cn/" target="_blank"
         rel="nofollow noopener">粤ICP备18109716号-1</a>
      
	  

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/mdbootstrap/js/jquery-3.4.1.min.js"></script>
<script src="/lib/mdbootstrap/js/popper.min.js"></script>
<script src="/lib/mdbootstrap/js/bootstrap.min.js?v=4.3.1"></script>
<script src="/lib/mdbootstrap/js/mdb.min.js?v=4.8.7"></script>
<script src="/lib/nprogress/nprogress.min.js?v=0.2.0"></script>
<script src="/js/main.js"></script>

  
    <script src="/lib/tocbot/tocbot.min.js?v=4.7.0"></script>
  
  <script src="/js/post.js"></script>


  <script src="/lib/prettify/prettify.min.js?v=0.1.0"></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint linenums');
      prettyPrint();
    })
  </script>


  <script src="/lib/typed/typed.min.js?v=2.0.9"></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "【学习日记】第一个爬虫---Scrapy框架入门(一)&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>


  <script src="/lib/anchor/anchor.min.js?v=4.2.0"></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js"></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>

</body>
</html>
